{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## River figure(s): cumulative flow, weekly or bi-monthly flow\n",
    "\n",
    "### Import modules and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lindsay\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/lindsay/'\n",
    "\n",
    "heeia_13 = pd.read_csv('heeia_13.csv', sep='\\t', header=None)\n",
    "heeia_13.columns = ['USGS', 'ID', 'Date', 'Zone', 'Heeia_13', 'A']\n",
    "heeia_13 = pd.DataFrame.drop(heeia_13, columns=['USGS', 'ID', 'Zone', 'A'])\n",
    "heeia_14 = pd.read_csv('heeia_14.csv', sep='\\t', header=None)\n",
    "heeia_14.columns = ['USGS', 'ID', 'Date', 'Zone', 'Heeia_14', 'A']\n",
    "heeia_14 = pd.DataFrame.drop(heeia_14, columns=['USGS', 'ID', 'Zone', 'A'])\n",
    "waiahole_13 = pd.read_csv('waiahole_13.csv', sep='\\t', header=None) \n",
    "waiahole_13.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waiahole_13', 'A']\n",
    "waiahole_13 = pd.DataFrame.drop(waiahole_13, columns=['USGS', 'ID', 'Zone', 'A'])\n",
    "waiahole_14 = pd.read_csv('waiahole_14.csv', sep='\\t', header=None)\n",
    "waiahole_14.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waiahole_14', 'A']\n",
    "waiahole_14 = pd.DataFrame.drop(waiahole_14, columns=['USGS', 'ID', 'Zone', 'A'])\n",
    "waihee_13 = pd.read_csv('waihee_13.csv', sep='\\t', header=None) \n",
    "waihee_13.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waihee_13', 'A']\n",
    "waihee_13 = pd.DataFrame.drop(waihee_13, columns=['USGS', 'ID', 'Zone', 'A'])                                \n",
    "waihee_14 = pd.read_csv('waihee_14.csv', sep='\\t', header=None) \n",
    "waihee_14.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waihee_14', 'A']\n",
    "waihee_14 = pd.DataFrame.drop(waihee_14, columns=['USGS', 'ID', 'Zone', 'A']) \n",
    "waikane_13 = pd.read_csv('waikane_13.csv', sep='\\t', header=None)\n",
    "waikane_13.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waikane_13', 'A']\n",
    "waikane_13 = pd.DataFrame.drop(waikane_13, columns=['USGS', 'ID', 'Zone', 'A']) \n",
    "waikane_14 = pd.read_csv('waikane_14.csv', sep='\\t', header=None) \n",
    "waikane_14.columns = ['USGS', 'ID', 'Date', 'Zone', 'Waikane_14', 'A']\n",
    "waikane_14 = pd.DataFrame.drop(waikane_14, columns=['USGS', 'ID', 'Zone', 'A']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Given the inconsistent data lengths and sporadic missing data, I'm going to start by merging all CSVs based on the Date column.\n",
    "\n",
    "Next, I'll separate the timestamps from the dates, then resample to mean daily values, calculate cumulative running sums, and finally, resample to mean weekly values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.merge(waiahole_13, waikane_13, on='Date', how='outer', sort = True)\n",
    "df_13 = pd.merge(df_13, waihee_13, on='Date', how='outer', sort = True)\n",
    "df_13 = pd.merge(df_13, heeia_13, on='Date', how='outer', sort = True)\n",
    "\n",
    "df_14 = pd.merge(waiahole_14, waikane_14, on='Date', how='outer', sort = True)\n",
    "df_14 = pd.merge(df_14, waihee_14, on='Date', how='outer', sort = True)\n",
    "df_14 = pd.merge(df_14, heeia_14, on='Date', how='outer', sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove timestamps\n",
    "df_13['Date'] = pd.to_datetime(df_13['Date']).dt.date\n",
    "df_14['Date'] = pd.to_datetime(df_14['Date']).dt.date\n",
    "# Classify date col as datetime object\n",
    "df_13['Date'] = pd.to_datetime(df_13['Date'])\n",
    "df_14['Date'] = pd.to_datetime(df_14['Date'])\n",
    "# Group by and average values by day\n",
    "df_13 = df_13.set_index('Date').groupby(pd.Grouper(freq='d')).mean().dropna(how='all')\n",
    "df_14 = df_14.set_index('Date').groupby(pd.Grouper(freq='d')).mean().dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative sums\n",
    "df_13['Waiahole_13_cs'] = df_13['Waiahole_13'].cumsum()\n",
    "df_13['Waikane_13_cs'] = df_13['Waikane_13'].cumsum()\n",
    "df_13['Waihee_13_cs'] = df_13['Waihee_13'].cumsum()\n",
    "df_13['Heeia_13_cs'] = df_13['Heeia_13'].cumsum()\n",
    "\n",
    "df_14['Waiahole_14_cs'] = df_14['Waiahole_14'].cumsum()\n",
    "df_14['Waikane_14_cs'] = df_14['Waikane_14'].cumsum()\n",
    "df_14['Waihee_14_cs'] = df_14['Waihee_14'].cumsum()\n",
    "df_14['Heeia_14_cs'] = df_14['Heeia_14'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly resampling\n",
    "df_13_weekly = df_13.resample('1W').mean().dropna(how='all')\n",
    "df_14_weekly = df_14.resample('1W').mean().dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot party\n",
    "\n",
    "For each year, I want to show the weekly discharge, and I also want to plot the running summed total for the year. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
